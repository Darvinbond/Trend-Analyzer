{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BzzvYQugTeh",
        "outputId": "e01d4295-bd57-46ef-8741-fea371b6ffd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VSHzAHlngVYS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "# figure(figsize=(20, 20), dpi=80)\n",
        "\n",
        "dataset = pd.read_excel(\"./drive/MyDrive/Data1.xlsx\")\n",
        "# dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NbNifN2QTtbR"
      },
      "outputs": [],
      "source": [
        "class OilDataset(Dataset):\n",
        "  def __init__(self, data):\n",
        "    # data loading\n",
        "    self.x = torch.from_numpy(data[:,:-1]).type(torch.FloatTensor)\n",
        "    self.y = torch.from_numpy(data[:,-1].reshape(-1, 1)).type(torch.FloatTensor)\n",
        "\n",
        "    self.n_samples = data.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # dataset index 0\n",
        "    return self.x[index], self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    # cll alen of dataset\n",
        "    return self.n_samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bnLPzA_LCVhg"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(nn.Module):\n",
        "  def __init__(self, n_input_features):\n",
        "    super(LogisticRegression, self).__init__()\n",
        "\n",
        "    self.linear1 = nn.Linear(n_input_features, 1)\n",
        "    # self.linear2 = nn.Linear(3, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_pred = self.linear1(x)\n",
        "    # y_pred = self.linear2(y_pred)\n",
        "    return y_pred\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fiH2CMC-EQ88"
      },
      "outputs": [],
      "source": [
        "from math import gamma\n",
        "class TrendAnalizer():\n",
        "  def __init__(self, dataset):\n",
        "    self.scale_data(dataset)\n",
        "\n",
        "  def train(self, model, train_loader, epochs):\n",
        "    print('Please Wait... ')\n",
        "    l_rate = 0.01\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=l_rate)\n",
        "\n",
        "    epochs = epochs\n",
        "    steps = len(train_loader)\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      for i, (x, y) in enumerate(train_loader):\n",
        "        inputs = x.to(device)\n",
        "        train_d = x.to(device)\n",
        "        labels = y.to(device)\n",
        "        train_yd = y.to(device)\n",
        "        \n",
        "        y_pred = model(inputs)\n",
        "        loss = criterion(y_pred, labels)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      if (epoch+1) % epochs == 0:\n",
        "        print(f'epoch: {epoch+1} / {epochs}, loss: {loss.item():.4f}')\n",
        "\n",
        "  def test(self, model, OilUS_test_dataset):\n",
        "    with torch.no_grad():\n",
        "      lenn = len(OilUS_test_dataset.x)\n",
        "      count = 0\n",
        "      pred = []\n",
        "      for inp in OilUS_test_dataset.x:\n",
        "        count += 1\n",
        "        out = model(inp)\n",
        "        pd = list(np.array(out.detach()).reshape(-1))\n",
        "\n",
        "        pred += pd\n",
        "\n",
        "        if count == lenn:\n",
        "          new_inp = inp\n",
        "          for i in range(12):\n",
        "            new_inp = new_inp[1:]\n",
        "            new_inp = torch.tensor(list(np.array(new_inp.detach())) + pd)\n",
        "            # print(new_inp)\n",
        "            # g\n",
        "\n",
        "            out = model(new_inp)\n",
        "            pd = list(np.array(out.detach()).reshape(-1))\n",
        "            # pd[0] = pd[0] + g\n",
        "            # print(pd[0])\n",
        "            # print(scaler)\n",
        "\n",
        "            pred += pd\n",
        "      return pred\n",
        "\n",
        "\n",
        "  def structureData(self, dataset):\n",
        "    dataset = dataset.reshape(len(dataset),)\n",
        "    ln = len(dataset)\n",
        "    i = 0\n",
        "    j = 6\n",
        "    counter_array = []\n",
        "    total_data = []\n",
        "    s = True\n",
        "    \n",
        "    while s:\n",
        "      for entry in dataset[i:j]:\n",
        "        counter_array.append(entry)\n",
        "        \n",
        "      if j == ln:\n",
        "        s = False\n",
        "      else:\n",
        "        i += 1\n",
        "        j += 1\n",
        "\n",
        "      total_data.append(counter_array)\n",
        "      counter_array = []\n",
        "    return np.array(total_data)\n",
        "\n",
        "    \n",
        "  def scale_data(self, dataset):\n",
        "    col = dataset.columns[1:]\n",
        "    list_of_col_data = {}\n",
        "\n",
        "    for i in col:\n",
        "      sc = StandardScaler()\n",
        "      c = np.array(dataset[i], dtype='float32').reshape(-1, 1)\n",
        "      c = sc.fit_transform(c)\n",
        "\n",
        "      list_of_col_data[i] = np.array([sc, c])\n",
        "\n",
        "    self.dataset = list_of_col_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qQG3u6WIlNyt"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "def plot(col_name, prediction, cut, all_pip, scaler):\n",
        "  new_prediction = []\n",
        "  for pred in prediction[-13:]:\n",
        "    g = random.randrange(-200, 200)\n",
        "    prd = np.array(pred).reshape(-1, 1)\n",
        "    print(prd)\n",
        "    # g = scaler.transform(g)\n",
        "    prd = scaler.inverse_transform(prd)\n",
        "    print(prd)\n",
        "    # g = g[0][0]\n",
        "    print('random')\n",
        "    print(g)\n",
        "    nw = prd[0][0] + g\n",
        "    print(\"Addition: \")\n",
        "    print(nw)\n",
        "    print('addition to scale: ')\n",
        "    neww = scaler.transform(np.array(nw).reshape(-1, 1))\n",
        "    print(neww)\n",
        "    # print(pred)\n",
        "    # print(\"+\")\n",
        "    # print(g)\n",
        "    # print(nw)\n",
        "    new_prediction.append(neww[0][0])\n",
        "  \n",
        "  # prediction = prediction[:-12] + new_prediction\n",
        "\n",
        "\n",
        "  deed1 = np.array(dataset[col_name][:-3])[cut:]\n",
        "  deed1 = np.array(deed1)\n",
        "  last_date = deed1[-1]\n",
        "\n",
        "  deed = [last_date,]\n",
        "\n",
        "  # print(type(deed))\n",
        "  for i in range(12):\n",
        "    date_after_month = last_date + relativedelta(months=1)\n",
        "    deed = np.append(deed, [date_after_month,])\n",
        "    last_date = date_after_month\n",
        "\n",
        "  plt.figure(figsize=(15,6))\n",
        "\n",
        "  \n",
        "  plt.figure(figsize=(15,6))\n",
        "  plt.plot(np.array(dataset[col_name][:-3])[:cut], all_pip[:cut], 'g')\n",
        "  plt.plot(np.array(dataset[col_name][:-3])[cut:], all_pip[cut:], 'b')\n",
        "  plt.plot(deed1, np.array(prediction[:-12]), 'r')\n",
        "  plt.plot(deed, np.array(new_prediction), 'y')\n",
        "  \n",
        "  plt.show()\n",
        "\n",
        "def plt_exact(col_name, predicted, cut, scaler):\n",
        "  new_prediction = []\n",
        "  for pred in predicted[-12:]:\n",
        "    g = random.randrange(-200, 200)\n",
        "    g = np.array(g).reshape(-1, 1)\n",
        "    g = scaler.transform(g)\n",
        "    g = g[0][0]\n",
        "    nw = pred + g\n",
        "    new_prediction.append(nw)\n",
        "\n",
        "  # predicted = predicted[:-12] + new_prediction\n",
        "\n",
        "  deed1 = np.array(dataset[col_name][:-3])[cut:]\n",
        "  deed1 = np.array(deed1)\n",
        "  last_date = deed1[-1]\n",
        "\n",
        "  deed = []\n",
        "\n",
        "  # print(type(deed))\n",
        "  for i in range(12):\n",
        "    date_after_month = last_date + relativedelta(months=1)\n",
        "    deed = np.append(deed, [date_after_month,])\n",
        "    last_date = date_after_month\n",
        "\n",
        "  plt.figure(figsize=(15,6))\n",
        "\n",
        "  plt.figure(figsize=(15,6))\n",
        "  plt.plot(np.array(dataset[col_name][:-3])[cut:], all_pip[cut:], 'b')\n",
        "  plt.plot(deed1, np.array(predicted[:-12]), 'r')\n",
        "  plt.plot(deed, np.array(new_prediction), 'y')\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXqsTcWdYsrZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "trend = TrendAnalizer(dataset[:-3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tefd0xGPlz5k"
      },
      "outputs": [],
      "source": [
        "config = {}\n",
        "\n",
        "for cols in dataset.columns[1:]:\n",
        "  data1 = None\n",
        "  scaler = None\n",
        "  \n",
        "  model = LogisticRegression(5)\n",
        "\n",
        "  data1 = trend.dataset[cols]\n",
        "\n",
        "  scaler = data1[0]\n",
        "\n",
        "  data_structured = trend.structureData(data1[1])\n",
        "\n",
        "  train_data_dataset = OilDataset(data_structured[:-80])\n",
        "  test_data_dataset = OilDataset(data_structured[-80:])\n",
        "\n",
        "  train_loader = DataLoader(dataset=train_data_dataset, batch_size=5, shuffle=False, num_workers=2)\n",
        "\n",
        "  trend.train(model, train_loader, 60)\n",
        "  \n",
        "  test_loader = DataLoader(dataset=test_data_dataset, batch_size=5, shuffle=False, num_workers=2)\n",
        "\n",
        "  predicted = trend.test(model, test_data_dataset)\n",
        "\n",
        "  all_pip = np.array(np.array(data1[1]).reshape(len(data1[1],)))\n",
        "\n",
        "  new_prediction = []\n",
        "\n",
        "  for pred in predicted[-13:]:\n",
        "    g = random.randrange(-200, 200)\n",
        "    prd = np.array(pred).reshape(-1, 1)\n",
        "    prd = scaler.inverse_transform(prd)\n",
        "    nw = prd[0][0] + g\n",
        "    neww = scaler.transform(np.array(nw).reshape(-1, 1))\n",
        "    \n",
        "    new_prediction.append(neww[0][0])\n",
        "    \n",
        "\n",
        "\n",
        "  deed1 = np.array(dataset['Date'][:-3])[-80:]\n",
        "  deed1 = np.array(deed1)\n",
        "  last_date = deed1[-1]\n",
        "\n",
        "  deed = [last_date,]\n",
        "  \n",
        "  for i in range(12):\n",
        "    date_after_month = last_date + relativedelta(months=1)\n",
        "    deed = np.append(deed, [date_after_month,])\n",
        "    last_date = date_after_month\n",
        "\n",
        "  plot_data = {\n",
        "    'x01':np.array(dataset[cols][:-3])[:-80],\n",
        "    'y01':all_pip[:-80],\n",
        "\n",
        "    'x02':np.array(dataset[cols][:-3])[-80:],\n",
        "    'y02':all_pip[-80:],\n",
        "\n",
        "    'x03':deed1,\n",
        "    'y03':np.array(predicted[:-12]),\n",
        "\n",
        "    'x04':deed,\n",
        "    'y04':np.array(new_prediction)\n",
        "  }\n",
        "\n",
        "  config[cols.replace(\" \", \"\")] = {plot_data}\n",
        "  torch.save(model.state_dict(), cols.replace(\" \", \"\")+\".pth\")\n",
        "  \n",
        "  plot('Date', \n",
        "      predicted,\n",
        "      -80, \n",
        "      all_pip,\n",
        "      scaler\n",
        "  )\n",
        "\n",
        "  plt_exact('Date', \n",
        "      predicted,\n",
        "      -80,\n",
        "      scaler\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8zgLt9kEmmvd"
      },
      "outputs": [],
      "source": [
        "configs = pd.DataFrame(config).transpose()\n",
        "\n",
        "dat = dict(configs['plot_data'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(dat, open('config.pickle', 'wb'))"
      ],
      "metadata": {
        "id": "0GTGMOloC8X4"
      },
      "execution_count": 29,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled22.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}